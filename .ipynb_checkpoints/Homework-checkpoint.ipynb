{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "colab_type": "text",
    "id": "GlrDP832IOsZ"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-716-8fb49f207874>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-716-8fb49f207874>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Extracting MFCC features from speech signals and classifying the emotions based on the features\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER NOTEBOOK FOR HOMEWORK 2 \n",
    "Extracting MFCC features from speech signals and classifying the emotions based on the features\n",
    "You may need to install librosa - check it out at https://librosa.org/doc/latest/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. The set of all required Python library imports have been set up for you; \n",
    "###  Feel to add any additional libraries you might want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1599777890719,
     "user": {
      "displayName": "Nikunj Rajesh Kotecha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giz2eVWmlNh1zjP20AbaTxZfckB5rNRoxmclty_=s64",
      "userId": "16556735897646851289"
     },
     "user_tz": 240
    },
    "id": "crt7LWmcAs5I"
   },
   "outputs": [],
   "source": [
    "import os, glob, time, sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from math import ceil\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "random.seed(45)\n",
    "audio_dir = \"myAudio\"\n",
    "data_dir = \"TESS\"\n",
    "\n",
    "\n",
    "## Collaborated with SHarwari Salunkhe ss3398@rit.edu for this homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsoeCiXiIJnz"
   },
   "source": [
    "## II. From the TESS folder, extract the filenamesand write them to file \n",
    "### Balance the data as best as possible. Split the 2800 .wav files into training=2240, validation=280 and testing=280,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the number of training, testing and validation splits\n",
    "# this function generates datasets that are as equally balanced as possible over the number of classes.\n",
    "# investigate the use of panda dataframes and the dataframe function 'groupby'\n",
    "# 'return train, test, valid' - three sorted dataframes \n",
    "#\n",
    "def get_data( dir, num_train, num_test, num_valid ):\n",
    "    \n",
    "    '***Insert your code here'\n",
    "    \n",
    "    df = {}\n",
    "    for folder in glob.iglob(dir+\"/*\"):\n",
    "        folder_list = []\n",
    "        for file in glob.iglob(folder+\"/*\"):\n",
    "            folder_list.append(file)\n",
    "        df[folder] = folder_list\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    num_test = int(num_test/14)\n",
    "    num_valid =int(num_valid/14)\n",
    "    test = df.sample(num_test,replace=False)\n",
    "    df.drop(test.index,inplace=True)\n",
    "    test = test.stack()\n",
    "    test = test.to_frame('filenames')\n",
    "    valid = df.sample(num_valid,replace=False)\n",
    "    df.drop(valid.index,inplace=True)\n",
    "    valid= valid.stack()\n",
    "    valid = valid.to_frame('filenames')\n",
    "    train = df.stack()\n",
    "    train = train.to_frame('filenames')\n",
    "    \n",
    "    return train['filenames'], test['filenames'], valid['filenames']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the different datasets here by calling 'get_data' and save them to CSV files (optional)\n",
    "# no need to add any code here...\n",
    "#\n",
    "tot_train, tot_test, tot_valid = 2240, 280, 280\n",
    "train_fnames, test_fnames, valid_fnames = get_data( data_dir, tot_train, tot_test, tot_valid )\n",
    "\n",
    "#save training, testing and validation csv's\n",
    "csv_train = 'train_filenames.csv'\n",
    "csv_test = 'test_filenames.csv'\n",
    "csv_valid = 'valid_filenames.csv'\n",
    "train_fnames.to_csv( csv_train, index=False )\n",
    "test_fnames.to_csv( csv_test, index=False )\n",
    "valid_fnames.to_csv( csv_valid, index=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Use librosa to extract features using the filenames;  save features to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculated values for durations of the .wav files\n",
    "# This has ben done for you already (we reviewed the max, min and mean durations of the .wav files)\n",
    "#\n",
    "minDur = 1.254\n",
    "avgDur = 2.055\n",
    "maxDur = 2.984\n",
    "categories = {'angry': [0],\n",
    "              'disgust': [1],\n",
    "              'fear': [2],\n",
    "              'happy': [3],\n",
    "              'neutral': [4],\n",
    "              'sad': [5],\n",
    "              'ps': [6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions here are used to standardize the input size of each .wav file. \n",
    "#  The waveform can be stretched using librosa.effects.time_stretch function,\n",
    "#  or can parts of the waveform can be replicated to pad the input.\n",
    "#\n",
    "def timeStretchWav(y, sr, target_duration):\n",
    "    '***Insert your code here'\n",
    "    pass\n",
    "    return y\n",
    "\n",
    "\n",
    "def replicateGrowWav(y, sr, target_duration):\n",
    "    '***Insert your code here'\n",
    "    k = ceil(target_duration * sr) - y.shape[0]\n",
    "    if k < 0:\n",
    "        y = y[:ceil(target_duration * sr)]\n",
    "    else:\n",
    "        y = np.pad(y,(0,k),mode='wrap')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Load in a filename and convert it to MFCC using the 'replicate' or 'stretch' method you wrote\n",
    "#\n",
    "def convertWavToMFCC(file_name, resize_duration):\n",
    "    '***Insert your code here'\n",
    "    y,sr = librosa.load(file_name)\n",
    "    mfcc = librosa.feature.mfcc(y= replicateGrowWav(y,sr,resize_duration),n_mfcc=13)\n",
    "    mfcc = mfcc.flatten()\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to extract the features for a given dataset\n",
    "#  The resulting X should be a stack of numpy arrays of MFCC features for each file in the dataset.\n",
    "#  The first column of the numpy array is the label string e.g. 'anger', 'sad', etc., and the rest   \n",
    "#  of the array is the 1677 flattened MFCC features. \n",
    "#\n",
    "def extractFeatures( csv_file, desc ):\n",
    "    '***Insert your code here'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['class'] = df['filenames'].apply(lambda x: x.split(\"_\")[-1].split(\".\")[0]);\n",
    "    df['mfcc'] = df['filenames'].apply(lambda x: convertWavToMFCC(x,2.984));\n",
    "    print(f'Done extracting for {desc}')\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-724-1e4a45b15b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcsv_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'valid_filenames.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mextractFeatures\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcsv_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Training -> '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractFeatures\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcsv_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Testing -> '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractFeatures\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcsv_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Validating -> '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-723-4b00c6d00a97>\u001b[0m in \u001b[0;36mextractFeatures\u001b[1;34m(csv_file, desc)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filenames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mfcc'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filenames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconvertWavToMFCC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2.984\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Done extracting for {desc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omkarsarde\\.conda\\envs\\torch\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-723-4b00c6d00a97>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filenames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mfcc'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filenames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconvertWavToMFCC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2.984\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Done extracting for {desc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-722-5d955b38ec26>\u001b[0m in \u001b[0;36mconvertWavToMFCC\u001b[1;34m(file_name, resize_duration)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvertWavToMFCC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize_duration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m'***Insert your code here'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmfcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mreplicateGrowWav\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresize_duration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_mfcc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmfcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmfcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omkarsarde\\.conda\\envs\\torch\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr_native\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omkarsarde\\.conda\\envs\\torch\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omkarsarde\\.conda\\envs\\torch\\lib\\site-packages\\resampy\\core.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mresample_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call 'extractFeatures' here to generate training, validation and testing features\n",
    "#  Write the features to .csv files so that they are easily read by the customized dataloader\n",
    "#  This has been written for you; no need to write anything here\n",
    "#\n",
    "\n",
    "csv_train = 'train_filenames.csv'\n",
    "csv_test = 'test_filenames.csv'\n",
    "csv_valid = 'valid_filenames.csv'\n",
    "\n",
    "X_train= extractFeatures( csv_train, 'Training -> ');\n",
    "X_test = extractFeatures( csv_test, 'Testing -> ');\n",
    "X_valid = extractFeatures( csv_valid, 'Validating -> ');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section implements a data loader useful for loading the MFCC features into memory for 'torch.nn' processing - You should not need to modify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set LSTM-related parameters\n",
    "# This has been setup for you (but feel free to modify)\n",
    "#\n",
    "input_size = 13\n",
    "h1 = 50\n",
    "output_dim = 7\n",
    "num_layers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lstm_style_batching(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    label = [item[1] for item in batch]\n",
    "    data = torch.cat(data, dim=1)\n",
    "    label = torch.cat(label, dim=0)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loader for MFCC speech data (reshapes the data such that input size is 13)\n",
    "class SpeechLoader(Dataset):\n",
    "    def __init__(self, dataset_file):\n",
    "        self.label = list()\n",
    "        self.dataset = list()\n",
    "        try:\n",
    "            df = dataset_file\n",
    "            for index in range(len(df)):\n",
    "                self.label.append(torch.tensor(categories[df.loc[index,'class']]))\n",
    "                np_array = np.array(df.loc[index,'mfcc'], dtype=np.float32).reshape(input_size, -1)\n",
    "                self.dataset.append(torch.from_numpy(np_array).permute(1, 0).reshape(-1, 1, input_size))\n",
    "        except FileNotFoundError:\n",
    "            print('generate features for [' + dataset_file + ']')\n",
    "            exit(1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.label[idx]\n",
    "\n",
    "    def to(self, device):\n",
    "        for i in range(len(self.dataset)):\n",
    "            self.label[i] = self.label[i].to(device=device)\n",
    "            self.dataset[i] = self.dataset[i].to(device=device)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the data loaders here on your saved features files as shown below\n",
    "#\n",
    "train_loader = DataLoader(SpeechLoader(X_train),batch_size=32,collate_fn=lstm_style_batching, shuffle=True)\n",
    "test_loader = DataLoader(SpeechLoader(X_test),batch_size=32, collate_fn=lstm_style_batching, shuffle=True)\n",
    "valid_loader = DataLoader(SpeechLoader(X_valid),batch_size=32, collate_fn=lstm_style_batching, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Implement the 'LSTMSpeechEmo' classification model with 13 input dims and 7 output dims. Choose your number of layers for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the LSTM class for speech emotion\n",
    "class LSTMSpeechEmo(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, target_size, num_lstm_layers):\n",
    "        '***Insert your code here'\n",
    "        super(LSTMSpeechEmo,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.target_size = target_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm = nn.LSTM(self.input_dim,self.hidden_dim,self.num_lstm_layers)\n",
    "        self.fc= nn.Linear(self.hidden_dim,self.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 =autograd.Variable(torch.zeros(self.num_lstm_layers, x.size(1), self.hidden_dim))\n",
    "        c_0 =autograd.Variable(torch.zeros(self.num_lstm_layers, x.size(1), self.hidden_dim))\n",
    "        out, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        out = h_out\n",
    "        print(out.shape,\"OUTPUT\")\n",
    "        output = out.view(out.size(1),-1)\n",
    "        output = self.fc(output)\n",
    "        print(output.shape)\n",
    "        return out\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function that takes a model (already trained) and a dataset (alread loaded) and \n",
    "#  calculates the metrics (loss, accuracy and confusion matrix) resulting from applying the \n",
    "#  model on the dataset. NOTE: DO NOT NORMALIZE YOUR CONFUSION MATRIX\n",
    "#\n",
    "def validation_metrics (model, dataset,nb_classes=7):\n",
    "    '***Insert your code here'\n",
    "    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    labellist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (inputs, classes) in enumerate(dataset):\n",
    "            inputs = inputs\n",
    "            classes = classes\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predlist=torch.cat([predlist,preds.view(-1)])\n",
    "            labellist=torch.cat([labellist,classes.view(-1)])\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "    print(conf_mat)\n",
    "\n",
    "    # Per-class accuracy\n",
    "    class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
    "    print(class_accuracy)\n",
    "    \n",
    "    return class_accuracy, confusion_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize the model, the loss function and the optimizer to be used\n",
    "# Write the code to train the model\n",
    "#\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = LSTMSpeechEmo(input_size, h1, output_dim, num_layers) \n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "'***Insert your code here'\n",
    "model.train()\n",
    "train_samples,test_samples,valid_samples = 2240,280,280\n",
    "epochs = 10\n",
    "history=[]\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    for ind,(x,y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(1)\n",
    "        ret, predictions = torch.max(output.data, 1)\n",
    "        correct_counts = predictions.eq(y.data.view_as(predictions))\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        train_acc += acc.item() * x.size(1)\n",
    "#         uncomment to view loss per batch\n",
    "#         print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(ind, loss.item(), acc.item()))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for ind, (x, y) in enumerate(valid_loader):\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            valid_loss += loss.item() * x.size(1)\n",
    "            ret, predictions = torch.max(output.data, 1)\n",
    "            correct_counts = predictions.eq(y.data.view_as(predictions))\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            valid_acc += acc.item() * x.size(1)\n",
    "#             uncomment to view loss per batch\n",
    "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(ind,loss.item(),acc.item()))\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Using  the function 'validation_metrics', accuracies and losses on the test dataset. Generate a confusion matrix for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call 'validation_metrics' on the test dataset and generate the confusion matrix \n",
    "#\n",
    "validation_metrics(model,test_loader);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Implement the code needed to load and test your own recorded voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually write your filenames in a .csv file. Extract the MFCC features, save and use data laoder to \n",
    "#  load for torch.nn processing. Apply the trained model on the loaded features.\n",
    "#  Call validation_metrics on your custom data and include the results in your PDF file. \n",
    "## \n",
    "'***Insert your code here'\n",
    "df = {}\n",
    "file_list = []\n",
    "for file in glob.iglob(audio_dir+\"/*\"):\n",
    "    file_list.append(file)\n",
    "df['filenames'] = file_list\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "df['labels'] = df['filenames'].apply(lambda x: x.split(\"_\")[0].split(\"\\\\\")[-1]);\n",
    "df['mfcc'] = df['filenames'].apply(lambda x: convertWavToMFCC(x,2.984,\"replicate\"));\n",
    "df.drop(columns=['filenames'],inplace=True)\n",
    "audio = DataLoader(SpeechLoader(df),batch_size=2,collate_fn=lstm_style_batching, shuffle=True)\n",
    "validation_metrics(model,audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFt0puQ4JCxKHcy/BAOuLi",
   "collapsed_sections": [],
   "name": "Agreement/Disagreement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
