{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1599777890719,
     "user": {
      "displayName": "Nikunj Rajesh Kotecha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giz2eVWmlNh1zjP20AbaTxZfckB5rNRoxmclty_=s64",
      "userId": "16556735897646851289"
     },
     "user_tz": 240
    },
    "id": "crt7LWmcAs5I"
   },
   "outputs": [],
   "source": [
    "import os, glob, time, sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from math import ceil\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "random.seed(45)\n",
    "audio_dir = \"myAudio\"\n",
    "data_dir = \"TESS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the number of training, testing and validation splits\n",
    "# this function generates datasets that are as equally balanced as possible over the number of classes.\n",
    "#\n",
    "\n",
    "def get_data( dir, num_train, num_test, num_valid ):    \n",
    "    df = {}\n",
    "    for folder in glob.iglob(dir+\"/*\"):\n",
    "        folder_list = []\n",
    "        for file in glob.iglob(folder+\"/*\"):\n",
    "            folder_list.append(file)\n",
    "        df[folder] = folder_list\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    num_test = int(num_test/14)\n",
    "    num_valid =int(num_valid/14)\n",
    "    test = df.sample(num_test,replace=False)\n",
    "    df.drop(test.index,inplace=True)\n",
    "    test = test.stack()\n",
    "    test = test.to_frame('filenames')\n",
    "    valid = df.sample(num_valid,replace=False)\n",
    "    df.drop(valid.index,inplace=True)\n",
    "    valid= valid.stack()\n",
    "    valid = valid.to_frame('filenames')\n",
    "    train = df.stack()\n",
    "    train = train.to_frame('filenames')\n",
    "    \n",
    "    return train['filenames'], test['filenames'], valid['filenames']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_train, tot_test, tot_valid = 2240, 280, 280\n",
    "train_fnames, test_fnames, valid_fnames = get_data( data_dir, tot_train, tot_test, tot_valid )\n",
    "\n",
    "#save training, testing and validation csv's\n",
    "csv_train = 'train_filenames.csv'\n",
    "csv_test = 'test_filenames.csv'\n",
    "csv_valid = 'valid_filenames.csv'\n",
    "train_fnames.to_csv( csv_train, index=False )\n",
    "test_fnames.to_csv( csv_test, index=False )\n",
    "valid_fnames.to_csv( csv_valid, index=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minDur = 1.254\n",
    "avgDur = 2.055\n",
    "maxDur = 2.984\n",
    "categories = {'angry': [0],\n",
    "              'disgust': [1],\n",
    "              'fear': [2],\n",
    "              'happy': [3],\n",
    "              'neutral': [4],\n",
    "              'sad': [5],\n",
    "              'ps': [6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize length\n",
    "def replicateGrowWav(y, sr, target_duration):\n",
    "    k = ceil(target_duration * sr) - y.shape[0]\n",
    "    if k < 0:\n",
    "        y = y[:ceil(target_duration * sr)]\n",
    "    else:\n",
    "        y = np.pad(y,(0,k),mode='wrap')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convertWavToMFCC(file_name, resize_duration):\n",
    "    y,sr = librosa.load(file_name)\n",
    "    mfcc = librosa.feature.mfcc(y= replicateGrowWav(y,sr,resize_duration),n_mfcc=13)\n",
    "    mfcc = mfcc.flatten()\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "def extractFeatures( csv_file, desc ):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['class'] = df['filenames'].apply(lambda x: x.split(\"_\")[-1].split(\".\")[0]);\n",
    "    df['mfcc'] = df['filenames'].apply(lambda x: convertWavToMFCC(x,2.984));\n",
    "    print(f'Done extracting for {desc}')\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting for Training -> \n",
      "Done extracting for Testing -> \n",
      "Done extracting for Validating -> \n"
     ]
    }
   ],
   "source": [
    "# Create train test valid\n",
    "csv_train = 'train_filenames.csv'\n",
    "csv_test = 'test_filenames.csv'\n",
    "csv_valid = 'valid_filenames.csv'\n",
    "\n",
    "X_train= extractFeatures( csv_train, 'Training -> ');\n",
    "X_test = extractFeatures( csv_test, 'Testing -> ');\n",
    "X_valid = extractFeatures( csv_valid, 'Validating -> ');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set LSTM-related parameters\n",
    "input_size = 13\n",
    "h1 = 50\n",
    "output_dim = 7\n",
    "num_layers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "def lstm_style_batching(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    label = [item[1] for item in batch]\n",
    "    data = torch.cat(data, dim=1)\n",
    "    label = torch.cat(label, dim=0)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loader for MFCC speech data (reshapes the data such that input size is 13)\n",
    "class SpeechLoader(Dataset):\n",
    "    def __init__(self, dataset_file):\n",
    "        self.label = list()\n",
    "        self.dataset = list()\n",
    "        try:\n",
    "            df = dataset_file\n",
    "            for index in range(len(df)):\n",
    "                self.label.append(torch.tensor(categories[df.loc[index,'class']]))\n",
    "                np_array = np.array(df.loc[index,'mfcc'], dtype=np.float32).reshape(input_size, -1)\n",
    "                self.dataset.append(torch.from_numpy(np_array).permute(1, 0).reshape(-1, 1, input_size))\n",
    "        except FileNotFoundError:\n",
    "            print('generate features for [' + dataset_file + ']')\n",
    "            exit(1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.label[idx]\n",
    "\n",
    "    def to(self, device):\n",
    "        for i in range(len(self.dataset)):\n",
    "            self.label[i] = self.label[i].to(device=device)\n",
    "            self.dataset[i] = self.dataset[i].to(device=device)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders\n",
    "train_loader = DataLoader(SpeechLoader(X_train),batch_size=32,collate_fn=lstm_style_batching, shuffle=True)\n",
    "test_loader = DataLoader(SpeechLoader(X_test),batch_size=32, collate_fn=lstm_style_batching, shuffle=True)\n",
    "valid_loader = DataLoader(SpeechLoader(X_valid),batch_size=32, collate_fn=lstm_style_batching, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the LSTM class for speech emotion\n",
    "class LSTMSpeechEmo(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, target_size, num_lstm_layers):\n",
    "        super(LSTMSpeechEmo,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.target_size = target_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm = nn.LSTM(self.input_dim,self.hidden_dim,self.num_lstm_layers)\n",
    "        self.fc= nn.Linear(self.hidden_dim,self.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 =autograd.Variable(torch.zeros(self.num_lstm_layers, x.size(1), self.hidden_dim))\n",
    "        c_0 =autograd.Variable(torch.zeros(self.num_lstm_layers, x.size(1), self.hidden_dim))\n",
    "        out, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        out = h_out\n",
    "        output = out.view(out.size(1),-1)\n",
    "        output = self.fc(output)\n",
    "        # BUG WAS HERE\n",
    "        # returned out INSTEAD OF output\n",
    "        # return out\n",
    "        return output\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "def validation_metrics (model, dataset,nb_classes=7):\n",
    "    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    labellist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    \n",
    "    labels =np.asarray(['angry','disgust','fear','happy','neutral','sad','ps'])\n",
    "    demo_conf_mat = np.vstack((labels,labels,labels,labels,labels,labels,labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (inputs, classes) in enumerate(dataset):\n",
    "            inputs = inputs\n",
    "            classes = classes\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predlist=torch.cat([predlist,preds.view(-1)])\n",
    "            labellist=torch.cat([labellist,classes.view(-1)])\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_mat=confusion_matrix(labellist.numpy(), predlist.numpy())\n",
    "    print(conf_mat)\n",
    "    print('READ CONFUSION MATRIX AS DEMO CONFUSION MATRIX')\n",
    "    print('DEMO CONFUSION MATRIX:')\n",
    "    print(demo_conf_mat)\n",
    "    # Per-class accuracy\n",
    "    class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
    "    total_acc = (conf_mat.sum() - (conf_mat.sum() - conf_mat.diagonal().sum())) / conf_mat.sum()\n",
    "    print('PER CLASS ACCURACY: ',class_accuracy)\n",
    "    print('Total ACCURACY',total_acc)\n",
    "    \n",
    "    return class_accuracy, conf_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Batch number: 000, Training: Loss: 1.9549, Accuracy: 0.1250\n",
      "Batch number: 001, Training: Loss: 1.9470, Accuracy: 0.1562\n",
      "Batch number: 002, Training: Loss: 1.9493, Accuracy: 0.1875\n",
      "Batch number: 003, Training: Loss: 1.9204, Accuracy: 0.2500\n",
      "Batch number: 004, Training: Loss: 1.9433, Accuracy: 0.1875\n",
      "Batch number: 005, Training: Loss: 1.9671, Accuracy: 0.1250\n",
      "Batch number: 006, Training: Loss: 1.9577, Accuracy: 0.1250\n",
      "Batch number: 007, Training: Loss: 1.9394, Accuracy: 0.1875\n",
      "Batch number: 008, Training: Loss: 1.8944, Accuracy: 0.1875\n",
      "Batch number: 009, Training: Loss: 1.9694, Accuracy: 0.1875\n",
      "Batch number: 010, Training: Loss: 1.9786, Accuracy: 0.1250\n",
      "Batch number: 011, Training: Loss: 1.8196, Accuracy: 0.3750\n",
      "Batch number: 012, Training: Loss: 1.8915, Accuracy: 0.4062\n",
      "Batch number: 013, Training: Loss: 1.9189, Accuracy: 0.2500\n",
      "Batch number: 014, Training: Loss: 1.8860, Accuracy: 0.2812\n",
      "Batch number: 015, Training: Loss: 1.8681, Accuracy: 0.3438\n",
      "Batch number: 016, Training: Loss: 1.8759, Accuracy: 0.3125\n",
      "Batch number: 017, Training: Loss: 1.9267, Accuracy: 0.1562\n",
      "Batch number: 018, Training: Loss: 1.8794, Accuracy: 0.3438\n",
      "Batch number: 019, Training: Loss: 1.8440, Accuracy: 0.3125\n",
      "Batch number: 020, Training: Loss: 1.8774, Accuracy: 0.3125\n",
      "Batch number: 021, Training: Loss: 1.8351, Accuracy: 0.3125\n",
      "Batch number: 022, Training: Loss: 1.8010, Accuracy: 0.3750\n",
      "Batch number: 023, Training: Loss: 1.8540, Accuracy: 0.3750\n",
      "Batch number: 024, Training: Loss: 1.8599, Accuracy: 0.3750\n",
      "Batch number: 025, Training: Loss: 1.8725, Accuracy: 0.2500\n",
      "Batch number: 026, Training: Loss: 1.7992, Accuracy: 0.4375\n",
      "Batch number: 027, Training: Loss: 1.8188, Accuracy: 0.3125\n",
      "Batch number: 028, Training: Loss: 1.8024, Accuracy: 0.4062\n",
      "Batch number: 029, Training: Loss: 1.8945, Accuracy: 0.2500\n",
      "Batch number: 030, Training: Loss: 1.7683, Accuracy: 0.4375\n",
      "Batch number: 031, Training: Loss: 1.7610, Accuracy: 0.5312\n",
      "Batch number: 032, Training: Loss: 1.8545, Accuracy: 0.3438\n",
      "Batch number: 033, Training: Loss: 1.7441, Accuracy: 0.5000\n",
      "Batch number: 034, Training: Loss: 1.8025, Accuracy: 0.3750\n",
      "Batch number: 035, Training: Loss: 1.8718, Accuracy: 0.1875\n",
      "Batch number: 036, Training: Loss: 1.8414, Accuracy: 0.1875\n",
      "Batch number: 037, Training: Loss: 1.7998, Accuracy: 0.3438\n",
      "Batch number: 038, Training: Loss: 1.7481, Accuracy: 0.4688\n",
      "Batch number: 039, Training: Loss: 1.7331, Accuracy: 0.4375\n",
      "Batch number: 040, Training: Loss: 1.7994, Accuracy: 0.3438\n",
      "Batch number: 041, Training: Loss: 1.7606, Accuracy: 0.3125\n",
      "Batch number: 042, Training: Loss: 1.7647, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 1.7156, Accuracy: 0.5000\n",
      "Batch number: 044, Training: Loss: 1.6898, Accuracy: 0.5000\n",
      "Batch number: 045, Training: Loss: 1.7996, Accuracy: 0.4062\n",
      "Batch number: 046, Training: Loss: 1.7739, Accuracy: 0.3125\n",
      "Batch number: 047, Training: Loss: 1.7531, Accuracy: 0.4688\n",
      "Batch number: 048, Training: Loss: 1.7555, Accuracy: 0.3750\n",
      "Batch number: 049, Training: Loss: 1.8063, Accuracy: 0.3438\n",
      "Batch number: 050, Training: Loss: 1.7844, Accuracy: 0.3438\n",
      "Batch number: 051, Training: Loss: 1.7591, Accuracy: 0.4375\n",
      "Batch number: 052, Training: Loss: 1.7564, Accuracy: 0.3438\n",
      "Batch number: 053, Training: Loss: 1.7381, Accuracy: 0.4062\n",
      "Batch number: 054, Training: Loss: 1.7874, Accuracy: 0.4375\n",
      "Batch number: 055, Training: Loss: 1.7319, Accuracy: 0.3750\n",
      "Batch number: 056, Training: Loss: 1.7635, Accuracy: 0.3750\n",
      "Batch number: 057, Training: Loss: 1.7944, Accuracy: 0.3438\n",
      "Batch number: 058, Training: Loss: 1.6612, Accuracy: 0.5625\n",
      "Batch number: 059, Training: Loss: 1.7569, Accuracy: 0.3438\n",
      "Batch number: 060, Training: Loss: 1.7766, Accuracy: 0.3125\n",
      "Batch number: 061, Training: Loss: 1.6808, Accuracy: 0.5000\n",
      "Batch number: 062, Training: Loss: 1.6918, Accuracy: 0.5312\n",
      "Batch number: 063, Training: Loss: 1.7386, Accuracy: 0.3750\n",
      "Batch number: 064, Training: Loss: 1.7299, Accuracy: 0.4375\n",
      "Batch number: 065, Training: Loss: 1.6547, Accuracy: 0.5625\n",
      "Batch number: 066, Training: Loss: 1.6377, Accuracy: 0.4688\n",
      "Batch number: 067, Training: Loss: 1.7001, Accuracy: 0.4062\n",
      "Batch number: 068, Training: Loss: 1.7082, Accuracy: 0.4062\n",
      "Batch number: 069, Training: Loss: 1.6780, Accuracy: 0.4375\n",
      "Validation Batch number: 000, Validation: Loss: 1.6754, Accuracy: 0.5000\n",
      "Validation Batch number: 001, Validation: Loss: 1.7236, Accuracy: 0.3750\n",
      "Validation Batch number: 002, Validation: Loss: 1.7240, Accuracy: 0.3125\n",
      "Validation Batch number: 003, Validation: Loss: 1.8048, Accuracy: 0.3750\n",
      "Validation Batch number: 004, Validation: Loss: 1.7890, Accuracy: 0.2812\n",
      "Validation Batch number: 005, Validation: Loss: 1.7297, Accuracy: 0.4688\n",
      "Validation Batch number: 006, Validation: Loss: 1.6006, Accuracy: 0.5625\n",
      "Validation Batch number: 007, Validation: Loss: 1.6760, Accuracy: 0.6250\n",
      "Validation Batch number: 008, Validation: Loss: 1.7167, Accuracy: 0.4583\n",
      "Epoch: 2/10\n",
      "Batch number: 000, Training: Loss: 1.7274, Accuracy: 0.4375\n",
      "Batch number: 001, Training: Loss: 1.6463, Accuracy: 0.5625\n",
      "Batch number: 002, Training: Loss: 1.6521, Accuracy: 0.4375\n",
      "Batch number: 003, Training: Loss: 1.7024, Accuracy: 0.3125\n",
      "Batch number: 004, Training: Loss: 1.7499, Accuracy: 0.4375\n",
      "Batch number: 005, Training: Loss: 1.6917, Accuracy: 0.4062\n",
      "Batch number: 006, Training: Loss: 1.6945, Accuracy: 0.5312\n",
      "Batch number: 007, Training: Loss: 1.6245, Accuracy: 0.5938\n",
      "Batch number: 008, Training: Loss: 1.6640, Accuracy: 0.5000\n",
      "Batch number: 009, Training: Loss: 1.6132, Accuracy: 0.6250\n",
      "Batch number: 010, Training: Loss: 1.6923, Accuracy: 0.3750\n",
      "Batch number: 011, Training: Loss: 1.6504, Accuracy: 0.4375\n",
      "Batch number: 012, Training: Loss: 1.6395, Accuracy: 0.5000\n",
      "Batch number: 013, Training: Loss: 1.6067, Accuracy: 0.5625\n",
      "Batch number: 014, Training: Loss: 1.6072, Accuracy: 0.5312\n",
      "Batch number: 015, Training: Loss: 1.5339, Accuracy: 0.5938\n",
      "Batch number: 016, Training: Loss: 1.5899, Accuracy: 0.5000\n",
      "Batch number: 017, Training: Loss: 1.6099, Accuracy: 0.4688\n",
      "Batch number: 018, Training: Loss: 1.6696, Accuracy: 0.4062\n",
      "Batch number: 019, Training: Loss: 1.6598, Accuracy: 0.5312\n",
      "Batch number: 020, Training: Loss: 1.5827, Accuracy: 0.6250\n",
      "Batch number: 021, Training: Loss: 1.5793, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 1.7222, Accuracy: 0.3438\n",
      "Batch number: 023, Training: Loss: 1.5916, Accuracy: 0.4375\n",
      "Batch number: 024, Training: Loss: 1.6563, Accuracy: 0.4062\n",
      "Batch number: 025, Training: Loss: 1.5958, Accuracy: 0.5000\n",
      "Batch number: 026, Training: Loss: 1.6962, Accuracy: 0.3750\n",
      "Batch number: 027, Training: Loss: 1.7204, Accuracy: 0.4062\n",
      "Batch number: 028, Training: Loss: 1.6341, Accuracy: 0.3125\n",
      "Batch number: 029, Training: Loss: 1.6511, Accuracy: 0.5000\n",
      "Batch number: 030, Training: Loss: 1.7086, Accuracy: 0.3438\n",
      "Batch number: 031, Training: Loss: 1.5378, Accuracy: 0.5938\n",
      "Batch number: 032, Training: Loss: 1.5258, Accuracy: 0.6562\n",
      "Batch number: 033, Training: Loss: 1.5719, Accuracy: 0.5000\n",
      "Batch number: 034, Training: Loss: 1.5567, Accuracy: 0.5938\n",
      "Batch number: 035, Training: Loss: 1.4992, Accuracy: 0.6562\n",
      "Batch number: 036, Training: Loss: 1.5980, Accuracy: 0.5000\n",
      "Batch number: 037, Training: Loss: 1.5617, Accuracy: 0.5625\n",
      "Batch number: 038, Training: Loss: 1.5745, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 1.4467, Accuracy: 0.6562\n",
      "Batch number: 040, Training: Loss: 1.6060, Accuracy: 0.4375\n",
      "Batch number: 041, Training: Loss: 1.5881, Accuracy: 0.4688\n",
      "Batch number: 042, Training: Loss: 1.6152, Accuracy: 0.4375\n",
      "Batch number: 043, Training: Loss: 1.6053, Accuracy: 0.5000\n",
      "Batch number: 044, Training: Loss: 1.4665, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 1.5443, Accuracy: 0.5312\n",
      "Batch number: 046, Training: Loss: 1.6186, Accuracy: 0.4062\n",
      "Batch number: 047, Training: Loss: 1.5010, Accuracy: 0.5938\n",
      "Batch number: 048, Training: Loss: 1.5908, Accuracy: 0.4375\n",
      "Batch number: 049, Training: Loss: 1.4664, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 1.5629, Accuracy: 0.4688\n",
      "Batch number: 051, Training: Loss: 1.6209, Accuracy: 0.4375\n",
      "Batch number: 052, Training: Loss: 1.5507, Accuracy: 0.4688\n",
      "Batch number: 053, Training: Loss: 1.4781, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 1.6611, Accuracy: 0.4062\n",
      "Batch number: 055, Training: Loss: 1.4904, Accuracy: 0.5938\n",
      "Batch number: 056, Training: Loss: 1.5730, Accuracy: 0.4688\n",
      "Batch number: 057, Training: Loss: 1.4650, Accuracy: 0.6875\n",
      "Batch number: 058, Training: Loss: 1.4046, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 1.5222, Accuracy: 0.5000\n",
      "Batch number: 060, Training: Loss: 1.3631, Accuracy: 0.6562\n",
      "Batch number: 061, Training: Loss: 1.6025, Accuracy: 0.4062\n",
      "Batch number: 062, Training: Loss: 1.4484, Accuracy: 0.5000\n",
      "Batch number: 063, Training: Loss: 1.3675, Accuracy: 0.6875\n",
      "Batch number: 064, Training: Loss: 1.5742, Accuracy: 0.4062\n",
      "Batch number: 065, Training: Loss: 1.4274, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 1.4381, Accuracy: 0.7500\n",
      "Batch number: 067, Training: Loss: 1.4425, Accuracy: 0.6562\n",
      "Batch number: 068, Training: Loss: 1.6179, Accuracy: 0.3750\n",
      "Batch number: 069, Training: Loss: 1.3779, Accuracy: 0.6250\n",
      "Validation Batch number: 000, Validation: Loss: 1.5315, Accuracy: 0.4062\n",
      "Validation Batch number: 001, Validation: Loss: 1.5904, Accuracy: 0.4375\n",
      "Validation Batch number: 002, Validation: Loss: 1.4922, Accuracy: 0.5312\n",
      "Validation Batch number: 003, Validation: Loss: 1.4780, Accuracy: 0.5625\n",
      "Validation Batch number: 004, Validation: Loss: 1.4085, Accuracy: 0.6562\n",
      "Validation Batch number: 005, Validation: Loss: 1.5834, Accuracy: 0.4062\n",
      "Validation Batch number: 006, Validation: Loss: 1.5123, Accuracy: 0.5625\n",
      "Validation Batch number: 007, Validation: Loss: 1.4573, Accuracy: 0.5938\n",
      "Validation Batch number: 008, Validation: Loss: 1.5552, Accuracy: 0.5417\n",
      "Epoch: 3/10\n",
      "Batch number: 000, Training: Loss: 1.4510, Accuracy: 0.5938\n",
      "Batch number: 001, Training: Loss: 1.4771, Accuracy: 0.6562\n",
      "Batch number: 002, Training: Loss: 1.5685, Accuracy: 0.3750\n",
      "Batch number: 003, Training: Loss: 1.4672, Accuracy: 0.5000\n",
      "Batch number: 004, Training: Loss: 1.3341, Accuracy: 0.6562\n",
      "Batch number: 005, Training: Loss: 1.3710, Accuracy: 0.5625\n",
      "Batch number: 006, Training: Loss: 1.4734, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 1.5879, Accuracy: 0.4375\n",
      "Batch number: 008, Training: Loss: 1.3795, Accuracy: 0.6875\n",
      "Batch number: 009, Training: Loss: 1.4892, Accuracy: 0.5312\n",
      "Batch number: 010, Training: Loss: 1.3878, Accuracy: 0.6250\n",
      "Batch number: 011, Training: Loss: 1.4297, Accuracy: 0.5938\n",
      "Batch number: 012, Training: Loss: 1.4755, Accuracy: 0.5312\n",
      "Batch number: 013, Training: Loss: 1.4247, Accuracy: 0.4688\n",
      "Batch number: 014, Training: Loss: 1.4875, Accuracy: 0.5312\n",
      "Batch number: 015, Training: Loss: 1.4419, Accuracy: 0.6250\n",
      "Batch number: 016, Training: Loss: 1.4795, Accuracy: 0.5312\n",
      "Batch number: 017, Training: Loss: 1.3890, Accuracy: 0.5625\n",
      "Batch number: 018, Training: Loss: 1.3482, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 1.3162, Accuracy: 0.6562\n",
      "Batch number: 020, Training: Loss: 1.4639, Accuracy: 0.5312\n",
      "Batch number: 021, Training: Loss: 1.4860, Accuracy: 0.5000\n",
      "Batch number: 022, Training: Loss: 1.5753, Accuracy: 0.4062\n",
      "Batch number: 023, Training: Loss: 1.3058, Accuracy: 0.6875\n",
      "Batch number: 024, Training: Loss: 1.3529, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 1.4255, Accuracy: 0.5000\n",
      "Batch number: 026, Training: Loss: 1.4803, Accuracy: 0.4375\n",
      "Batch number: 027, Training: Loss: 1.4225, Accuracy: 0.5000\n",
      "Batch number: 028, Training: Loss: 1.4452, Accuracy: 0.4688\n",
      "Batch number: 029, Training: Loss: 1.5150, Accuracy: 0.4062\n",
      "Batch number: 030, Training: Loss: 1.3717, Accuracy: 0.6562\n",
      "Batch number: 031, Training: Loss: 1.4302, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 1.2762, Accuracy: 0.6562\n",
      "Batch number: 033, Training: Loss: 1.3429, Accuracy: 0.6250\n",
      "Batch number: 034, Training: Loss: 1.3930, Accuracy: 0.5000\n",
      "Batch number: 035, Training: Loss: 1.3525, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 1.3682, Accuracy: 0.5625\n",
      "Batch number: 037, Training: Loss: 1.2229, Accuracy: 0.7500\n",
      "Batch number: 038, Training: Loss: 1.3947, Accuracy: 0.6250\n",
      "Batch number: 039, Training: Loss: 1.3098, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 1.4023, Accuracy: 0.6562\n",
      "Batch number: 041, Training: Loss: 1.3473, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 1.5057, Accuracy: 0.5000\n",
      "Batch number: 043, Training: Loss: 1.3757, Accuracy: 0.5625\n",
      "Batch number: 044, Training: Loss: 1.3240, Accuracy: 0.7188\n",
      "Batch number: 045, Training: Loss: 1.3641, Accuracy: 0.5312\n",
      "Batch number: 046, Training: Loss: 1.3479, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 1.1666, Accuracy: 0.7812\n",
      "Batch number: 048, Training: Loss: 1.2845, Accuracy: 0.6875\n",
      "Batch number: 049, Training: Loss: 1.3769, Accuracy: 0.5625\n",
      "Batch number: 050, Training: Loss: 1.3603, Accuracy: 0.5312\n",
      "Batch number: 051, Training: Loss: 1.3075, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 1.3540, Accuracy: 0.7188\n",
      "Batch number: 053, Training: Loss: 1.3123, Accuracy: 0.6562\n",
      "Batch number: 054, Training: Loss: 1.1944, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 1.3234, Accuracy: 0.7188\n",
      "Batch number: 056, Training: Loss: 1.3200, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 1.3175, Accuracy: 0.6875\n",
      "Batch number: 058, Training: Loss: 1.3230, Accuracy: 0.7188\n",
      "Batch number: 059, Training: Loss: 1.1736, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 1.2868, Accuracy: 0.6562\n",
      "Batch number: 061, Training: Loss: 1.2965, Accuracy: 0.5938\n",
      "Batch number: 062, Training: Loss: 1.3678, Accuracy: 0.5625\n",
      "Batch number: 063, Training: Loss: 1.1587, Accuracy: 0.7188\n",
      "Batch number: 064, Training: Loss: 1.4151, Accuracy: 0.5625\n",
      "Batch number: 065, Training: Loss: 1.4229, Accuracy: 0.5625\n",
      "Batch number: 066, Training: Loss: 1.3234, Accuracy: 0.6875\n",
      "Batch number: 067, Training: Loss: 1.3517, Accuracy: 0.7188\n",
      "Batch number: 068, Training: Loss: 1.2483, Accuracy: 0.7188\n",
      "Batch number: 069, Training: Loss: 1.2439, Accuracy: 0.7812\n",
      "Validation Batch number: 000, Validation: Loss: 1.2907, Accuracy: 0.6250\n",
      "Validation Batch number: 001, Validation: Loss: 1.3803, Accuracy: 0.5312\n",
      "Validation Batch number: 002, Validation: Loss: 1.3756, Accuracy: 0.5938\n",
      "Validation Batch number: 003, Validation: Loss: 1.1220, Accuracy: 0.8750\n",
      "Validation Batch number: 004, Validation: Loss: 1.4001, Accuracy: 0.4688\n",
      "Validation Batch number: 005, Validation: Loss: 1.2127, Accuracy: 0.8125\n",
      "Validation Batch number: 006, Validation: Loss: 1.3111, Accuracy: 0.5625\n",
      "Validation Batch number: 007, Validation: Loss: 1.2926, Accuracy: 0.7188\n",
      "Validation Batch number: 008, Validation: Loss: 1.2876, Accuracy: 0.6667\n",
      "Epoch: 4/10\n",
      "Batch number: 000, Training: Loss: 1.2746, Accuracy: 0.7188\n",
      "Batch number: 001, Training: Loss: 1.4563, Accuracy: 0.5000\n",
      "Batch number: 002, Training: Loss: 1.2707, Accuracy: 0.7500\n",
      "Batch number: 003, Training: Loss: 1.3300, Accuracy: 0.6562\n",
      "Batch number: 004, Training: Loss: 0.9496, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 1.2501, Accuracy: 0.6562\n",
      "Batch number: 006, Training: Loss: 1.2976, Accuracy: 0.5938\n",
      "Batch number: 007, Training: Loss: 1.2621, Accuracy: 0.6250\n",
      "Batch number: 008, Training: Loss: 1.2063, Accuracy: 0.7812\n",
      "Batch number: 009, Training: Loss: 1.1312, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 1.2590, Accuracy: 0.7188\n",
      "Batch number: 011, Training: Loss: 1.1515, Accuracy: 0.7500\n",
      "Batch number: 012, Training: Loss: 1.1595, Accuracy: 0.7812\n",
      "Batch number: 013, Training: Loss: 1.2709, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 1.2175, Accuracy: 0.7188\n",
      "Batch number: 015, Training: Loss: 1.2143, Accuracy: 0.7500\n",
      "Batch number: 016, Training: Loss: 1.1914, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 1.1050, Accuracy: 0.7812\n",
      "Batch number: 018, Training: Loss: 1.1777, Accuracy: 0.8438\n",
      "Batch number: 019, Training: Loss: 1.1544, Accuracy: 0.7812\n",
      "Batch number: 020, Training: Loss: 1.1952, Accuracy: 0.6875\n",
      "Batch number: 021, Training: Loss: 1.1657, Accuracy: 0.7500\n",
      "Batch number: 022, Training: Loss: 1.1421, Accuracy: 0.8125\n",
      "Batch number: 023, Training: Loss: 1.2211, Accuracy: 0.6875\n",
      "Batch number: 024, Training: Loss: 1.1313, Accuracy: 0.7188\n",
      "Batch number: 025, Training: Loss: 1.2287, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 1.1607, Accuracy: 0.7500\n",
      "Batch number: 027, Training: Loss: 1.1986, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 1.1811, Accuracy: 0.7500\n",
      "Batch number: 029, Training: Loss: 1.0557, Accuracy: 0.8438\n",
      "Batch number: 030, Training: Loss: 1.1533, Accuracy: 0.6875\n",
      "Batch number: 031, Training: Loss: 1.1963, Accuracy: 0.7188\n",
      "Batch number: 032, Training: Loss: 1.2487, Accuracy: 0.5000\n",
      "Batch number: 033, Training: Loss: 1.1865, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 1.0520, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 1.0612, Accuracy: 0.9062\n",
      "Batch number: 036, Training: Loss: 1.1727, Accuracy: 0.6875\n",
      "Batch number: 037, Training: Loss: 1.1723, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 1.1334, Accuracy: 0.8125\n",
      "Batch number: 039, Training: Loss: 1.1931, Accuracy: 0.6562\n",
      "Batch number: 040, Training: Loss: 1.0524, Accuracy: 0.8125\n",
      "Batch number: 041, Training: Loss: 1.0087, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 1.1795, Accuracy: 0.6562\n",
      "Batch number: 043, Training: Loss: 1.1771, Accuracy: 0.6250\n",
      "Batch number: 044, Training: Loss: 1.2125, Accuracy: 0.6562\n",
      "Batch number: 045, Training: Loss: 1.1646, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 1.1018, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 1.0987, Accuracy: 0.7188\n",
      "Batch number: 048, Training: Loss: 1.0431, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 1.2108, Accuracy: 0.7500\n",
      "Batch number: 050, Training: Loss: 1.0818, Accuracy: 0.7188\n",
      "Batch number: 051, Training: Loss: 1.0897, Accuracy: 0.8438\n",
      "Batch number: 052, Training: Loss: 0.9761, Accuracy: 0.8125\n",
      "Batch number: 053, Training: Loss: 1.1677, Accuracy: 0.7188\n",
      "Batch number: 054, Training: Loss: 1.0455, Accuracy: 0.7188\n",
      "Batch number: 055, Training: Loss: 1.1159, Accuracy: 0.7812\n",
      "Batch number: 056, Training: Loss: 0.9438, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 1.0628, Accuracy: 0.7188\n",
      "Batch number: 058, Training: Loss: 1.0775, Accuracy: 0.7812\n",
      "Batch number: 059, Training: Loss: 1.1980, Accuracy: 0.6562\n",
      "Batch number: 060, Training: Loss: 1.0089, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 1.0492, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 0.9410, Accuracy: 0.8438\n",
      "Batch number: 063, Training: Loss: 1.0356, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 1.1772, Accuracy: 0.6875\n",
      "Batch number: 065, Training: Loss: 1.0938, Accuracy: 0.7188\n",
      "Batch number: 066, Training: Loss: 1.1874, Accuracy: 0.5938\n",
      "Batch number: 067, Training: Loss: 1.1342, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.9821, Accuracy: 0.8438\n",
      "Batch number: 069, Training: Loss: 0.9559, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 1.0508, Accuracy: 0.6875\n",
      "Validation Batch number: 001, Validation: Loss: 0.9533, Accuracy: 0.8438\n",
      "Validation Batch number: 002, Validation: Loss: 1.0061, Accuracy: 0.8438\n",
      "Validation Batch number: 003, Validation: Loss: 1.1365, Accuracy: 0.7188\n",
      "Validation Batch number: 004, Validation: Loss: 0.9969, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 1.2657, Accuracy: 0.6875\n",
      "Validation Batch number: 006, Validation: Loss: 0.9627, Accuracy: 0.8750\n",
      "Validation Batch number: 007, Validation: Loss: 0.9180, Accuracy: 0.8125\n",
      "Validation Batch number: 008, Validation: Loss: 1.0338, Accuracy: 0.7500\n",
      "Epoch: 5/10\n",
      "Batch number: 000, Training: Loss: 1.0235, Accuracy: 0.7188\n",
      "Batch number: 001, Training: Loss: 0.9341, Accuracy: 0.7500\n",
      "Batch number: 002, Training: Loss: 0.9598, Accuracy: 0.7500\n",
      "Batch number: 003, Training: Loss: 1.0312, Accuracy: 0.7812\n",
      "Batch number: 004, Training: Loss: 0.9904, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 1.0755, Accuracy: 0.7500\n",
      "Batch number: 006, Training: Loss: 0.9150, Accuracy: 0.8125\n",
      "Batch number: 007, Training: Loss: 1.0623, Accuracy: 0.7500\n",
      "Batch number: 008, Training: Loss: 0.9188, Accuracy: 0.8125\n",
      "Batch number: 009, Training: Loss: 0.9656, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 1.0448, Accuracy: 0.7812\n",
      "Batch number: 011, Training: Loss: 0.9938, Accuracy: 0.8438\n",
      "Batch number: 012, Training: Loss: 0.8783, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.9341, Accuracy: 0.7812\n",
      "Batch number: 014, Training: Loss: 0.8337, Accuracy: 0.9062\n",
      "Batch number: 015, Training: Loss: 0.9932, Accuracy: 0.7188\n",
      "Batch number: 016, Training: Loss: 0.9189, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 0.8962, Accuracy: 0.7812\n",
      "Batch number: 018, Training: Loss: 0.9813, Accuracy: 0.6875\n",
      "Batch number: 019, Training: Loss: 0.8133, Accuracy: 0.9062\n",
      "Batch number: 020, Training: Loss: 0.9050, Accuracy: 0.8438\n",
      "Batch number: 021, Training: Loss: 0.9066, Accuracy: 0.7500\n",
      "Batch number: 022, Training: Loss: 0.8700, Accuracy: 0.7812\n",
      "Batch number: 023, Training: Loss: 0.8216, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.8746, Accuracy: 0.7500\n",
      "Batch number: 025, Training: Loss: 1.1056, Accuracy: 0.6562\n",
      "Batch number: 026, Training: Loss: 0.7958, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.9235, Accuracy: 0.8438\n",
      "Batch number: 028, Training: Loss: 0.9058, Accuracy: 0.7812\n",
      "Batch number: 029, Training: Loss: 0.8973, Accuracy: 0.7812\n",
      "Batch number: 030, Training: Loss: 0.7745, Accuracy: 0.8438\n",
      "Batch number: 031, Training: Loss: 0.9646, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 1.1504, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.8487, Accuracy: 0.8438\n",
      "Batch number: 034, Training: Loss: 0.8584, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.8757, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 1.0772, Accuracy: 0.7188\n",
      "Batch number: 037, Training: Loss: 0.8250, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.6634, Accuracy: 0.9688\n",
      "Batch number: 039, Training: Loss: 0.9051, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.7474, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.6937, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.9141, Accuracy: 0.7812\n",
      "Batch number: 043, Training: Loss: 0.7669, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.9001, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.9365, Accuracy: 0.7812\n",
      "Batch number: 046, Training: Loss: 0.8270, Accuracy: 0.9062\n",
      "Batch number: 047, Training: Loss: 0.8644, Accuracy: 0.7188\n",
      "Batch number: 048, Training: Loss: 0.8991, Accuracy: 0.7812\n",
      "Batch number: 049, Training: Loss: 0.8414, Accuracy: 0.7812\n",
      "Batch number: 050, Training: Loss: 0.8294, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 0.8905, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.8200, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.8370, Accuracy: 0.8438\n",
      "Batch number: 054, Training: Loss: 0.7931, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.8593, Accuracy: 0.9062\n",
      "Batch number: 056, Training: Loss: 0.8614, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.9728, Accuracy: 0.8125\n",
      "Batch number: 058, Training: Loss: 0.8337, Accuracy: 0.8125\n",
      "Batch number: 059, Training: Loss: 0.6949, Accuracy: 0.9688\n",
      "Batch number: 060, Training: Loss: 0.6884, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.6808, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.8127, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.7483, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.7037, Accuracy: 0.8438\n",
      "Batch number: 065, Training: Loss: 0.8154, Accuracy: 0.8438\n",
      "Batch number: 066, Training: Loss: 0.7652, Accuracy: 0.8438\n",
      "Batch number: 067, Training: Loss: 0.8828, Accuracy: 0.7812\n",
      "Batch number: 068, Training: Loss: 0.6674, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.9456, Accuracy: 0.7812\n",
      "Validation Batch number: 000, Validation: Loss: 0.9590, Accuracy: 0.7812\n",
      "Validation Batch number: 001, Validation: Loss: 0.7563, Accuracy: 0.8438\n",
      "Validation Batch number: 002, Validation: Loss: 0.8247, Accuracy: 0.7812\n",
      "Validation Batch number: 003, Validation: Loss: 0.7456, Accuracy: 0.9062\n",
      "Validation Batch number: 004, Validation: Loss: 0.8242, Accuracy: 0.8750\n",
      "Validation Batch number: 005, Validation: Loss: 0.7088, Accuracy: 0.9375\n",
      "Validation Batch number: 006, Validation: Loss: 0.7591, Accuracy: 0.8125\n",
      "Validation Batch number: 007, Validation: Loss: 0.7577, Accuracy: 0.8438\n",
      "Validation Batch number: 008, Validation: Loss: 0.9766, Accuracy: 0.8333\n",
      "Epoch: 6/10\n",
      "Batch number: 000, Training: Loss: 0.7117, Accuracy: 0.7812\n",
      "Batch number: 001, Training: Loss: 0.7803, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.7590, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 0.8076, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.7423, Accuracy: 0.8438\n",
      "Batch number: 005, Training: Loss: 0.6621, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 0.5906, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.7985, Accuracy: 0.9062\n",
      "Batch number: 008, Training: Loss: 0.6988, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 0.6565, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.7217, Accuracy: 0.9062\n",
      "Batch number: 011, Training: Loss: 0.6236, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.7441, Accuracy: 0.7500\n",
      "Batch number: 013, Training: Loss: 0.8091, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 0.7653, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.6831, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.7052, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 0.8095, Accuracy: 0.7500\n",
      "Batch number: 018, Training: Loss: 0.8245, Accuracy: 0.7500\n",
      "Batch number: 019, Training: Loss: 0.6619, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.7005, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.8207, Accuracy: 0.7188\n",
      "Batch number: 022, Training: Loss: 0.6581, Accuracy: 0.9062\n",
      "Batch number: 023, Training: Loss: 0.7486, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.6840, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.7212, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 0.5258, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.7372, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.8510, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.8306, Accuracy: 0.7500\n",
      "Batch number: 030, Training: Loss: 0.7312, Accuracy: 0.8438\n",
      "Batch number: 031, Training: Loss: 0.6711, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 0.7209, Accuracy: 0.8438\n",
      "Batch number: 033, Training: Loss: 0.7203, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.7317, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 0.8345, Accuracy: 0.7188\n",
      "Batch number: 036, Training: Loss: 0.7296, Accuracy: 0.9062\n",
      "Batch number: 037, Training: Loss: 0.5292, Accuracy: 0.9688\n",
      "Batch number: 038, Training: Loss: 0.6399, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.6092, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.7490, Accuracy: 0.8438\n",
      "Batch number: 041, Training: Loss: 0.6751, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.6443, Accuracy: 0.9062\n",
      "Batch number: 043, Training: Loss: 0.6084, Accuracy: 0.9062\n",
      "Batch number: 044, Training: Loss: 0.6910, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.5927, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.5157, Accuracy: 0.9062\n",
      "Batch number: 047, Training: Loss: 0.5864, Accuracy: 0.9062\n",
      "Batch number: 048, Training: Loss: 0.5874, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.6095, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.7815, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 0.7810, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.7218, Accuracy: 0.8125\n",
      "Batch number: 053, Training: Loss: 0.6288, Accuracy: 0.8438\n",
      "Batch number: 054, Training: Loss: 0.5553, Accuracy: 0.9062\n",
      "Batch number: 055, Training: Loss: 0.4846, Accuracy: 0.9062\n",
      "Batch number: 056, Training: Loss: 0.6734, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.5473, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 0.6623, Accuracy: 0.9062\n",
      "Batch number: 059, Training: Loss: 0.6225, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.6376, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 0.7265, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 0.5552, Accuracy: 0.9062\n",
      "Batch number: 063, Training: Loss: 0.4307, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.5851, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.6382, Accuracy: 0.8438\n",
      "Batch number: 066, Training: Loss: 0.7702, Accuracy: 0.7812\n",
      "Batch number: 067, Training: Loss: 0.6104, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.7831, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.5494, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.6307, Accuracy: 0.8750\n",
      "Validation Batch number: 001, Validation: Loss: 0.7529, Accuracy: 0.8125\n",
      "Validation Batch number: 002, Validation: Loss: 0.7244, Accuracy: 0.8438\n",
      "Validation Batch number: 003, Validation: Loss: 0.7892, Accuracy: 0.8750\n",
      "Validation Batch number: 004, Validation: Loss: 0.5944, Accuracy: 0.8125\n",
      "Validation Batch number: 005, Validation: Loss: 0.6780, Accuracy: 0.9062\n",
      "Validation Batch number: 006, Validation: Loss: 0.6981, Accuracy: 0.9375\n",
      "Validation Batch number: 007, Validation: Loss: 0.7345, Accuracy: 0.8750\n",
      "Validation Batch number: 008, Validation: Loss: 0.7318, Accuracy: 0.8333\n",
      "Epoch: 7/10\n",
      "Batch number: 000, Training: Loss: 0.7443, Accuracy: 0.7500\n",
      "Batch number: 001, Training: Loss: 0.5863, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.5465, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 0.7017, Accuracy: 0.8438\n",
      "Batch number: 004, Training: Loss: 0.5479, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.5883, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.5988, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.5765, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.5986, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 0.5050, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 0.5906, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.6051, Accuracy: 0.8438\n",
      "Batch number: 012, Training: Loss: 0.6885, Accuracy: 0.8125\n",
      "Batch number: 013, Training: Loss: 0.6463, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 0.5855, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.4590, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.5031, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 0.6142, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.5306, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.5956, Accuracy: 0.8438\n",
      "Batch number: 020, Training: Loss: 0.5875, Accuracy: 0.9062\n",
      "Batch number: 021, Training: Loss: 0.3498, Accuracy: 0.9688\n",
      "Batch number: 022, Training: Loss: 0.5800, Accuracy: 0.9062\n",
      "Batch number: 023, Training: Loss: 0.5302, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.6000, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 0.5470, Accuracy: 0.9062\n",
      "Batch number: 026, Training: Loss: 0.6145, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.6193, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 0.6630, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 0.6155, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.5619, Accuracy: 0.8438\n",
      "Batch number: 031, Training: Loss: 0.4056, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.7222, Accuracy: 0.8438\n",
      "Batch number: 033, Training: Loss: 0.5952, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.6028, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.5675, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.5302, Accuracy: 0.9688\n",
      "Batch number: 037, Training: Loss: 0.5893, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.4463, Accuracy: 0.9688\n",
      "Batch number: 039, Training: Loss: 0.6989, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 0.4949, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.6217, Accuracy: 0.8438\n",
      "Batch number: 042, Training: Loss: 0.5710, Accuracy: 0.9062\n",
      "Batch number: 043, Training: Loss: 0.5034, Accuracy: 0.9062\n",
      "Batch number: 044, Training: Loss: 0.6444, Accuracy: 0.8438\n",
      "Batch number: 045, Training: Loss: 0.5694, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.5759, Accuracy: 0.9062\n",
      "Batch number: 047, Training: Loss: 0.4417, Accuracy: 0.9688\n",
      "Batch number: 048, Training: Loss: 0.6548, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 0.5131, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.6358, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 0.5319, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 0.4443, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.6576, Accuracy: 0.8125\n",
      "Batch number: 054, Training: Loss: 0.5387, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.6038, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.4178, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.7874, Accuracy: 0.7500\n",
      "Batch number: 058, Training: Loss: 0.5490, Accuracy: 0.9062\n",
      "Batch number: 059, Training: Loss: 0.5193, Accuracy: 0.9062\n",
      "Batch number: 060, Training: Loss: 0.5148, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.5548, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.5611, Accuracy: 0.9062\n",
      "Batch number: 063, Training: Loss: 0.6894, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.5101, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 0.3329, Accuracy: 0.9688\n",
      "Batch number: 066, Training: Loss: 0.5450, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.4498, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.4845, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.5588, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 0.5689, Accuracy: 0.9062\n",
      "Validation Batch number: 001, Validation: Loss: 0.5534, Accuracy: 0.8750\n",
      "Validation Batch number: 002, Validation: Loss: 0.5849, Accuracy: 0.8438\n",
      "Validation Batch number: 003, Validation: Loss: 0.6333, Accuracy: 0.9375\n",
      "Validation Batch number: 004, Validation: Loss: 0.5591, Accuracy: 0.8750\n",
      "Validation Batch number: 005, Validation: Loss: 0.4184, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.5614, Accuracy: 0.9062\n",
      "Validation Batch number: 007, Validation: Loss: 0.5201, Accuracy: 0.8125\n",
      "Validation Batch number: 008, Validation: Loss: 0.5473, Accuracy: 0.9583\n",
      "Epoch: 8/10\n",
      "Batch number: 000, Training: Loss: 0.4225, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.5163, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.5815, Accuracy: 0.8438\n",
      "Batch number: 003, Training: Loss: 0.5632, Accuracy: 0.9062\n",
      "Batch number: 004, Training: Loss: 0.5513, Accuracy: 0.9062\n",
      "Batch number: 005, Training: Loss: 0.5587, Accuracy: 0.8125\n",
      "Batch number: 006, Training: Loss: 0.4428, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.3939, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.4159, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 0.4362, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.4294, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.5229, Accuracy: 0.9062\n",
      "Batch number: 012, Training: Loss: 0.5492, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.5381, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 0.5234, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.3742, Accuracy: 0.9688\n",
      "Batch number: 016, Training: Loss: 0.3618, Accuracy: 0.9688\n",
      "Batch number: 017, Training: Loss: 0.4641, Accuracy: 0.9688\n",
      "Batch number: 018, Training: Loss: 0.5562, Accuracy: 0.8438\n",
      "Batch number: 019, Training: Loss: 0.5610, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.5360, Accuracy: 0.9688\n",
      "Batch number: 021, Training: Loss: 0.4274, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.4730, Accuracy: 0.9062\n",
      "Batch number: 023, Training: Loss: 0.3795, Accuracy: 0.9688\n",
      "Batch number: 024, Training: Loss: 0.4207, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.4764, Accuracy: 0.9062\n",
      "Batch number: 026, Training: Loss: 0.4178, Accuracy: 0.9688\n",
      "Batch number: 027, Training: Loss: 0.4446, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.5387, Accuracy: 0.8438\n",
      "Batch number: 029, Training: Loss: 0.4727, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.5340, Accuracy: 0.8438\n",
      "Batch number: 031, Training: Loss: 0.4358, Accuracy: 0.9688\n",
      "Batch number: 032, Training: Loss: 0.3601, Accuracy: 0.9688\n",
      "Batch number: 033, Training: Loss: 0.4671, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.6410, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.4714, Accuracy: 0.9688\n",
      "Batch number: 036, Training: Loss: 0.4068, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.4681, Accuracy: 0.9688\n",
      "Batch number: 038, Training: Loss: 0.5092, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.3665, Accuracy: 0.9688\n",
      "Batch number: 040, Training: Loss: 0.5261, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 0.4902, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.4707, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.5100, Accuracy: 0.8438\n",
      "Batch number: 044, Training: Loss: 0.4613, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.4768, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.3918, Accuracy: 0.9688\n",
      "Batch number: 047, Training: Loss: 0.3096, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.4915, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.4345, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.5106, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 0.4598, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 0.4580, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 0.6003, Accuracy: 0.8438\n",
      "Batch number: 054, Training: Loss: 0.3565, Accuracy: 0.9688\n",
      "Batch number: 055, Training: Loss: 0.5425, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 0.4570, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.4780, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 0.5451, Accuracy: 0.8438\n",
      "Batch number: 059, Training: Loss: 0.3071, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.3933, Accuracy: 0.9688\n",
      "Batch number: 061, Training: Loss: 0.4438, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.5095, Accuracy: 0.8125\n",
      "Batch number: 063, Training: Loss: 0.4745, Accuracy: 0.8438\n",
      "Batch number: 064, Training: Loss: 0.4472, Accuracy: 0.9688\n",
      "Batch number: 065, Training: Loss: 0.3143, Accuracy: 0.9688\n",
      "Batch number: 066, Training: Loss: 0.6053, Accuracy: 0.8438\n",
      "Batch number: 067, Training: Loss: 0.4971, Accuracy: 0.9062\n",
      "Batch number: 068, Training: Loss: 0.4050, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.3240, Accuracy: 1.0000\n",
      "Validation Batch number: 000, Validation: Loss: 0.5085, Accuracy: 0.8750\n",
      "Validation Batch number: 001, Validation: Loss: 0.5488, Accuracy: 0.9062\n",
      "Validation Batch number: 002, Validation: Loss: 0.3153, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.5237, Accuracy: 0.8438\n",
      "Validation Batch number: 004, Validation: Loss: 0.5061, Accuracy: 0.9062\n",
      "Validation Batch number: 005, Validation: Loss: 0.3703, Accuracy: 0.9062\n",
      "Validation Batch number: 006, Validation: Loss: 0.4450, Accuracy: 0.8750\n",
      "Validation Batch number: 007, Validation: Loss: 0.6183, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.4602, Accuracy: 0.9167\n",
      "Epoch: 9/10\n",
      "Batch number: 000, Training: Loss: 0.4753, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.3486, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.3372, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.5216, Accuracy: 0.8438\n",
      "Batch number: 004, Training: Loss: 0.4615, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.3355, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.4593, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.2982, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.3522, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.4017, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.4384, Accuracy: 0.9062\n",
      "Batch number: 011, Training: Loss: 0.4326, Accuracy: 0.9688\n",
      "Batch number: 012, Training: Loss: 0.5236, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 0.3800, Accuracy: 0.9688\n",
      "Batch number: 014, Training: Loss: 0.4566, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.4356, Accuracy: 0.9688\n",
      "Batch number: 016, Training: Loss: 0.3730, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 0.2778, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.4231, Accuracy: 0.9688\n",
      "Batch number: 019, Training: Loss: 0.3347, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.4509, Accuracy: 0.9062\n",
      "Batch number: 021, Training: Loss: 0.3519, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.3110, Accuracy: 0.9688\n",
      "Batch number: 023, Training: Loss: 0.3832, Accuracy: 0.9062\n",
      "Batch number: 024, Training: Loss: 0.4011, Accuracy: 0.9062\n",
      "Batch number: 025, Training: Loss: 0.5590, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 0.6678, Accuracy: 0.7500\n",
      "Batch number: 027, Training: Loss: 0.3414, Accuracy: 0.9688\n",
      "Batch number: 028, Training: Loss: 0.3562, Accuracy: 0.9688\n",
      "Batch number: 029, Training: Loss: 0.4512, Accuracy: 0.8438\n",
      "Batch number: 030, Training: Loss: 0.2912, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.3280, Accuracy: 0.9688\n",
      "Batch number: 032, Training: Loss: 0.3260, Accuracy: 0.9688\n",
      "Batch number: 033, Training: Loss: 0.3614, Accuracy: 0.9062\n",
      "Batch number: 034, Training: Loss: 0.3221, Accuracy: 0.9688\n",
      "Batch number: 035, Training: Loss: 0.4202, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.3502, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.2907, Accuracy: 0.9688\n",
      "Batch number: 038, Training: Loss: 0.3537, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.4101, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.2871, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.3377, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 0.3478, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.3669, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.3430, Accuracy: 0.9688\n",
      "Batch number: 045, Training: Loss: 0.5977, Accuracy: 0.9062\n",
      "Batch number: 046, Training: Loss: 0.2885, Accuracy: 0.9688\n",
      "Batch number: 047, Training: Loss: 0.3247, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.3496, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.4171, Accuracy: 0.9062\n",
      "Batch number: 050, Training: Loss: 0.3411, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.2954, Accuracy: 0.9688\n",
      "Batch number: 052, Training: Loss: 0.3590, Accuracy: 0.9688\n",
      "Batch number: 053, Training: Loss: 0.4535, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 0.3750, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.4495, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.2894, Accuracy: 0.9688\n",
      "Batch number: 057, Training: Loss: 0.3403, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.3713, Accuracy: 0.9062\n",
      "Batch number: 059, Training: Loss: 0.2708, Accuracy: 0.9688\n",
      "Batch number: 060, Training: Loss: 0.3064, Accuracy: 0.9688\n",
      "Batch number: 061, Training: Loss: 0.3334, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.3355, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.3523, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.2977, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.3582, Accuracy: 0.9688\n",
      "Batch number: 066, Training: Loss: 0.3160, Accuracy: 0.9688\n",
      "Batch number: 067, Training: Loss: 0.3264, Accuracy: 0.9688\n",
      "Batch number: 068, Training: Loss: 0.3100, Accuracy: 0.9062\n",
      "Batch number: 069, Training: Loss: 0.4427, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.3579, Accuracy: 0.9062\n",
      "Validation Batch number: 001, Validation: Loss: 0.5319, Accuracy: 0.8750\n",
      "Validation Batch number: 002, Validation: Loss: 0.3003, Accuracy: 0.9688\n",
      "Validation Batch number: 003, Validation: Loss: 0.4117, Accuracy: 0.9375\n",
      "Validation Batch number: 004, Validation: Loss: 0.3592, Accuracy: 0.9688\n",
      "Validation Batch number: 005, Validation: Loss: 0.4528, Accuracy: 0.9062\n",
      "Validation Batch number: 006, Validation: Loss: 0.4228, Accuracy: 0.8750\n",
      "Validation Batch number: 007, Validation: Loss: 0.3456, Accuracy: 0.9688\n",
      "Validation Batch number: 008, Validation: Loss: 0.4426, Accuracy: 0.9583\n",
      "Epoch: 10/10\n",
      "Batch number: 000, Training: Loss: 0.3357, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.2814, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.4315, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 0.3554, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.2974, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.3093, Accuracy: 0.9688\n",
      "Batch number: 006, Training: Loss: 0.3009, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.3866, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.4088, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 0.3573, Accuracy: 0.9688\n",
      "Batch number: 010, Training: Loss: 0.3276, Accuracy: 0.9688\n",
      "Batch number: 011, Training: Loss: 0.2741, Accuracy: 0.9688\n",
      "Batch number: 012, Training: Loss: 0.4098, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.2420, Accuracy: 0.9688\n",
      "Batch number: 014, Training: Loss: 0.3430, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.2727, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.3626, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 0.3698, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.3505, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.3455, Accuracy: 0.9688\n",
      "Batch number: 020, Training: Loss: 0.3233, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.4646, Accuracy: 0.9688\n",
      "Batch number: 022, Training: Loss: 0.2624, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.2284, Accuracy: 0.9688\n",
      "Batch number: 024, Training: Loss: 0.2953, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.3393, Accuracy: 0.9688\n",
      "Batch number: 026, Training: Loss: 0.2736, Accuracy: 0.9688\n",
      "Batch number: 027, Training: Loss: 0.2221, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.2687, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.3143, Accuracy: 0.9688\n",
      "Batch number: 030, Training: Loss: 0.3038, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.4083, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.3303, Accuracy: 0.9688\n",
      "Batch number: 033, Training: Loss: 0.2872, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.3522, Accuracy: 0.9688\n",
      "Batch number: 035, Training: Loss: 0.3178, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.3356, Accuracy: 0.9062\n",
      "Batch number: 037, Training: Loss: 0.3767, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.2348, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.2860, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.4060, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.3419, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 0.3286, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.4103, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.3437, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.3630, Accuracy: 0.9688\n",
      "Batch number: 046, Training: Loss: 0.3467, Accuracy: 0.9688\n",
      "Batch number: 047, Training: Loss: 0.2870, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.1857, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1705, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.2527, Accuracy: 0.9688\n",
      "Batch number: 051, Training: Loss: 0.2059, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.2367, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.2907, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 0.3355, Accuracy: 0.9688\n",
      "Batch number: 055, Training: Loss: 0.1852, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.3396, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.4650, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.3517, Accuracy: 0.9688\n",
      "Batch number: 059, Training: Loss: 0.2914, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.3047, Accuracy: 0.9688\n",
      "Batch number: 061, Training: Loss: 0.4211, Accuracy: 0.9062\n",
      "Batch number: 062, Training: Loss: 0.2693, Accuracy: 0.9688\n",
      "Batch number: 063, Training: Loss: 0.3020, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.2302, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.3603, Accuracy: 0.9688\n",
      "Batch number: 066, Training: Loss: 0.3327, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.2774, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.2103, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 0.2569, Accuracy: 0.9688\n",
      "Validation Batch number: 000, Validation: Loss: 0.3581, Accuracy: 0.9688\n",
      "Validation Batch number: 001, Validation: Loss: 0.2547, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.4092, Accuracy: 0.9375\n",
      "Validation Batch number: 003, Validation: Loss: 0.3065, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 0.2867, Accuracy: 1.0000\n",
      "Validation Batch number: 005, Validation: Loss: 0.3054, Accuracy: 0.9062\n",
      "Validation Batch number: 006, Validation: Loss: 0.4472, Accuracy: 0.9375\n",
      "Validation Batch number: 007, Validation: Loss: 0.2520, Accuracy: 0.9375\n",
      "Validation Batch number: 008, Validation: Loss: 0.3131, Accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = LSTMSpeechEmo(input_size, h1, output_dim, num_layers) \n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "model.train()\n",
    "train_samples,test_samples,valid_samples = 2240,280,280\n",
    "epochs = 10\n",
    "history=[]\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    for ind,(x,y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(1)\n",
    "        ret, predictions = torch.max(output.data, 1)\n",
    "        correct_counts = predictions.eq(y.data.view_as(predictions))\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        train_acc += acc.item() * x.size(1)\n",
    "#         uncomment to view loss per batch\n",
    "        print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(ind, loss.item(), acc.item()))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for ind, (x, y) in enumerate(valid_loader):\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            valid_loss += loss.item() * x.size(1)\n",
    "            ret, predictions = torch.max(output.data, 1)\n",
    "            correct_counts = predictions.eq(y.data.view_as(predictions))\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            valid_acc += acc.item() * x.size(1)\n",
    "#             uncomment to view loss per batch\n",
    "            print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(ind,loss.item(),acc.item()))\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  0  0  0  0  0  0]\n",
      " [ 0 39  0  0  1  0  0]\n",
      " [ 0  1 38  1  0  0  0]\n",
      " [ 0  0  0 39  0  0  1]\n",
      " [ 0  2  0  0 37  1  0]\n",
      " [ 0  0  0  0  4 36  0]\n",
      " [ 0  1  0  1  0  0 38]]\n",
      "READ CONFUSION MATRIX AS DEMO CONFUSION MATRIX\n",
      "DEMO CONFUSION MATRIX:\n",
      "[['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'ps']\n",
      " ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'ps']\n",
      " ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'ps']\n",
      " ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'ps']\n",
      " ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'ps']\n",
      " ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'ps']\n",
      " ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'ps']]\n",
      "PER CLASS ACCURACY:  [100.   97.5  95.   97.5  92.5  90.   95. ]\n",
      "Total ACCURACY 0.9535714285714286\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix and performance metrics\n",
    "validation_metrics(model,test_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFt0puQ4JCxKHcy/BAOuLi",
   "collapsed_sections": [],
   "name": "Agreement/Disagreement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
